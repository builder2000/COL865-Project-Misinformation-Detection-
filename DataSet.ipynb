{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from sklearn import utils as skutils\n",
    "from gensim import utils\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_csv = \"D:\\\\Courses\\\\Sem 8 2021-22\\\\COL865\\\\Project Dataset\\\\CodaLab\\\\Constraint_English_Train - Sheet1.csv\"\n",
    "Test_csv = \"D:\\\\Courses\\\\Sem 8 2021-22\\\\COL865\\\\Project Dataset\\\\CodaLab\\\\english_test_with_labels - Sheet1.csv\"\n",
    "Val_csv = \"D:\\\\Courses\\\\Sem 8 2021-22\\\\COL865\\\\Project Dataset\\\\CodaLab\\\\Constraint_English_Val - Sheet1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=[]):\n",
    "    \n",
    "\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \" \") \n",
    "\n",
    "    string = string.lower()\n",
    "\n",
    "\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X)\n",
    "    stddev = np.std(X)\n",
    "    return (X - mean) * (1 / stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC2VEC Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(clean_text(remove_emojis(row)).split(), [index]) for index, row in enumerate(df_x)]\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skutils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(clean_text(row).split())\n",
    "                                     for index, row in enumerate(df_x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2141176.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2143050.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2136589.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2137097.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3216367.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2170691.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2117602.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2142539.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3213680.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1605020.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3199172.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3267099.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3265118.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1603969.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3198792.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3208320.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2140155.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3204120.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3210232.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3210232.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# way to use doc2vec transformer\n",
    "\n",
    "# can i train it on a a  larger corpus ?\n",
    "# look into it.\n",
    "df_x = df['tweet']\n",
    "doc2vec = Doc2VecTransformer(vector_size = 2000)\n",
    "doc2vec_model =  doc2vec.fit(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for TF-IDF \n",
    "#unigram\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# # bigram\n",
    "# vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "\n",
    "# # unigram + bigram\n",
    "# vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "\n",
    "\n",
    "#for Count Vectorizer\n",
    "# unigram\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1))\n",
    "\n",
    "\n",
    "# performance has been best for CountVectorizer\n",
    "\n",
    "def dataset(pathname, model = 'TF_IDF', default = 'test', vectorizer = vectorizer):\n",
    "    \n",
    "    \"\"\"\n",
    "    model : TF_IDF or Doc_term or Doc2Vec\n",
    "    default : train or test\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(pathname)\n",
    "\n",
    "    if(model == 'TF_IDF' or model == 'Doc_term'):\n",
    "        corpus = []\n",
    "        for x in df['tweet']:\n",
    "            corpus.append(clean_text(remove_emojis(x)))\n",
    "        \n",
    "        if(default == 'train'):\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "        elif(default == 'test'):\n",
    "            X = vectorizer.transform(corpus)\n",
    "        \n",
    "        Y = np.array([1 if y == 'real' else 0 for y in df['label']])\n",
    "        \n",
    "        #print(X.shape, Y.shape)   \n",
    "        return X, Y\n",
    "    \n",
    "    elif(model == 'Doc2Vec'):\n",
    "        if(default == 'train'):\n",
    "            X = doc2vec_model.transform(df['tweet'])\n",
    "        elif(default == 'test'):\n",
    "            X = doc2vec_model.transform(df['tweet'])\n",
    "\n",
    "        Y = [1 if y == 'real' else 0 for y in df['label']]\n",
    "        \n",
    "        pca = PCA(n_components=500)\n",
    "        X = pca.fit_transform(X)\n",
    "        print(X.shape)      \n",
    "        return X, Y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = dataset(Train_csv , model = 'Doc2Vec', default = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(clf.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, y):\n",
    "    acc = 0\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == y[i]:\n",
    "            acc +=1\n",
    "    return acc/len(Y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5308411214953271"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 500)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test =  dataset(Test_csv, model = 'Doc2Vec' , default = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred = [int(clf.predict(X_test[i, :].reshape(1, -1))) for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5275700934579439"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = dataset(Train_csv , model = 'TF_IDF', default = 'train')\n",
    "X_test, Y_test =  dataset(Val_csv, model = 'TF_IDF' , default = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "y_pred = [int(clf.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947040498442368"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9219626168224299"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(clf.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "# rfc will take huge time to converge for so many features, may be truncated SVD to decrease number of features\n",
    "# tune hyper-parameters to improve test accuracy\n",
    "RFC = rfc(random_state=0).fit(X_train, Y_train)\n",
    "y_pred = [int(RFC.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "# overfitting clearly !, anyway expected\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922429906542056"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(RFC.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = rfc()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 90, 100, 110],\n",
       "                         'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5],\n",
       "                         'min_samples_split': [8, 10, 12],\n",
       "                         'n_estimators': [100, 200, 300, 1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = [int(best_grid.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs using various kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan Debbarma\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:57:27] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'logloss',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "\n",
    "xgb1 = xgb.XGBClassifier()\n",
    "\n",
    "xgbmodel = xgb1.fit(X_train, Y_train)\n",
    "y_pred = [int(xgbmodel.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9537383177570093"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098130841121496"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(xgbmodel.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (128, 64) , random_state = 1,  max_iter = 300, learning_rate = 'adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpmodel = mlp.fit(X_train, Y_train)\n",
    "y_pred = [int(mlpmodel.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285046728971963"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(mlpmodel.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into RNNs and CNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
