{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from sklearn import utils as skutils\n",
    "from gensim import utils\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_csv = \"D:\\\\Courses\\\\Sem 8 2021-22\\\\COL865\\\\Project Dataset\\\\CodaLab\\\\Constraint_English_Train - Sheet1.csv\"\n",
    "Test_csv = \"D:\\\\Courses\\\\Sem 8 2021-22\\\\COL865\\\\Project Dataset\\\\CodaLab\\\\english_test_with_labels - Sheet1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=[]):\n",
    "    \n",
    "\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \" \") \n",
    "\n",
    "    string = string.lower()\n",
    "\n",
    "\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X)\n",
    "    stddev = np.std(X)\n",
    "    return (X - mean) * (1 / stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC2VEC Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(clean_text(remove_emojis(row)).split(), [index]) for index, row in enumerate(df_x)]\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skutils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(clean_text(row).split())\n",
    "                                     for index, row in enumerate(df_x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3210232.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3206410.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2144586.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3238416.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2138455.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1283481.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1620377.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3204120.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3200312.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1604542.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3211764.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3212530.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2139474.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3213297.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1604733.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3214448.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 2112452.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3211764.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 3209084.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 6420/6420 [00:00<00:00, 1284950.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# way to use doc2vec transformer\n",
    "\n",
    "# can i train it on a a  larger corpus ?\n",
    "# look into it.\n",
    "df_x = df['tweet']\n",
    "doc2vec = Doc2VecTransformer(vector_size = 2000)\n",
    "doc2vec_model =  doc2vec.fit(df_x)\n",
    "doc2vec_features = doc2vec_model.transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "def dataset(pathname, model = 'TF_IDF', default = 'test', vectorizer = vectorizer):\n",
    "    \n",
    "    \"\"\"\n",
    "    model : TF_IDF or Doc_term or Doc2Vec\n",
    "    default : train or test\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(pathname)\n",
    "\n",
    "    if(model == 'TF_IDF' or model == 'Doc_term'):\n",
    "        corpus = []\n",
    "        for x in df['tweet']:\n",
    "            corpus.append(clean_text(remove_emojis(x)))\n",
    "        \n",
    "        if(default == 'train'):\n",
    "            X = vectorizer.fit_transform(corpus)\n",
    "        elif(default == 'test'):\n",
    "            X = vectorizer.transform(corpus)\n",
    "        \n",
    "        Y = np.array([1 if y == 'real' else 0 for y in df['label']])\n",
    "        \n",
    "        #print(X.shape, Y.shape)   \n",
    "        return X, Y\n",
    "    \n",
    "    elif(model == 'Doc2Vec'):\n",
    "        if(default == 'train'):\n",
    "            X = doc2vec_model.transform(df['tweet'])\n",
    "        elif(default == 'test'):\n",
    "            X = doc2vec_model.transform(df['tweet'])\n",
    "\n",
    "        Y = [1 if y == 'real' else 0 for y in df['label']]\n",
    "        \n",
    "        pca = PCA(n_components=500)\n",
    "        X = pca.fit_transform(X)\n",
    "        print(X.shape)      \n",
    "        return X, Y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = dataset(Train_csv , model = 'Doc2Vec', default = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(clf.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, y):\n",
    "    acc = 0\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == y[i]:\n",
    "            acc +=1\n",
    "    return acc/len(Y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347352024922118"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 500)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test =  dataset(Test_csv, model = 'Doc2Vec' , default = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred = [int(clf.predict(X_test[i, :].reshape(1, -1))) for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.530373831775701"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF features + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = dataset(Train_csv , model = 'TF_IDF', default = 'train')\n",
    "X_test, Y_test =  dataset(Test_csv, model = 'TF_IDF' , default = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "y_pred = [int(clf.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947040498442368"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9257009345794392"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(clf.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF features + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "# rfc will take huge time to converge for so many features, may be truncated SVD to decrease number of features\n",
    "# tune hyper-parameters to improve test accuracy\n",
    "RFC = rfc(random_state=0).fit(X_train, Y_train)\n",
    "y_pred = [int(RFC.predict(X_train[i, :].reshape(1, -1))) for i in range(6420)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "# overfitting clearly !, anyway expected\n",
    "accuracy(Y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9233644859813084"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred = [int(clf.predict(X_test[i, :].reshape(1, -1))) for i in range(2140)]\n",
    "accuracy(Y_test, ytest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs using various kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into RNNs and CNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
